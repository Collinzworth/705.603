{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Collect Stop words\n",
    "all_stopwords = stopwords.words('english')\n",
    "print(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "stopwords_within = []\n",
    "corpus = []\n",
    "num_lines = len(dataset)\n",
    "\n",
    "for i in range(0, num_lines):\n",
    "\n",
    "    # Get words only using regex in re\n",
    "    line = dataset[\"Review\"].iloc[i]\n",
    "    line_words = re.findall(\"[A-Za-z']+\", line)\n",
    "    \n",
    "    # Make all lower case\n",
    "    line_words_lower = [None]*len(line_words)\n",
    "    for index, word in enumerate(line_words):\n",
    "        line_words_lower[index] = word.lower()\n",
    "    \n",
    "    # print(line_words_lower)\n",
    "\n",
    "    # Split into an array of strings necessary to remove stop words\n",
    "    line_stopwords = []\n",
    "    for index, word in enumerate(line_words):\n",
    "        if word in all_stopwords:\n",
    "            line_stopwords.append(word)\n",
    "\n",
    "    influence_stopwords = [\"no\", \"not\", \"did\", \"didn't\", \"only\", \"above\", \"had\", \"hadn't\"]\n",
    "    all_stopwords_adjusted = [stop_word for stop_word in all_stopwords if i not in influence_stopwords]\n",
    "\n",
    "    # Add back in words that may influence the sentiment such as 'not'\n",
    "    # Split into an array of strings necessary to remove stop words\n",
    "    line_words_no_stop_words = [word for word in line_words if word not in all_stopwords_adjusted]\n",
    "\n",
    "    # Stem the words and filter out stopwords\n",
    "    stemmer = PorterStemmer()\n",
    "    porter_stemmed = [stemmer.stem(word) for word in line_words_no_stop_words]\n",
    "\n",
    "    # Turn back into a string\n",
    "    review = \" \".join(porter_stemmed) \n",
    "\n",
    "    # append the string to the total\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(dataset):\n",
    "\n",
    "    words = []\n",
    "    stopwords_within = []\n",
    "    corpus = []\n",
    "    num_lines = len(dataset)\n",
    "\n",
    "    for i in range(0, num_lines):\n",
    "\n",
    "        # Get words only using regex in re\n",
    "        line = dataset[\"Review\"].iloc[i]\n",
    "        line_words = re.findall(\"[A-Za-z']+\", line)\n",
    "        \n",
    "        # Make all lower case\n",
    "        line_words_lower = [None]*len(line_words)\n",
    "        for index, word in enumerate(line_words):\n",
    "            line_words_lower[index] = word.lower()\n",
    "        \n",
    "        # print(line_words_lower)\n",
    "\n",
    "        # Split into an array of strings necessary to remove stop words\n",
    "        line_stopwords = []\n",
    "        for index, word in enumerate(line_words):\n",
    "            if word in all_stopwords:\n",
    "                line_stopwords.append(word)\n",
    "\n",
    "        influence_stopwords = [\"no\", \"not\", \"did\", \"didn't\", \"only\", \"above\", \"had\", \"hadn't\"]\n",
    "        all_stopwords_adjusted = [stop_word for stop_word in all_stopwords if i not in influence_stopwords]\n",
    "\n",
    "        # Add back in words that may influence the sentiment such as 'not'\n",
    "        # Split into an array of strings necessary to remove stop words\n",
    "        line_words_no_stop_words = [word for word in line_words if word not in all_stopwords_adjusted]\n",
    "\n",
    "        # Stem the words and filter out stopwords\n",
    "        stemmer = PorterStemmer()\n",
    "        porter_stemmed = [stemmer.stem(word) for word in line_words_no_stop_words]\n",
    "\n",
    "        # Turn back into a string\n",
    "        review = \" \".join(porter_stemmed) \n",
    "\n",
    "        # append the string to the total\n",
    "        corpus.append(review)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review  Liked\n",
      "0                             Wow... Loved this place.      1\n",
      "1                                   Crust is not good.      0\n",
      "2            Not tasty and the texture was just nasty.      0\n",
      "3    Stopped by during the late May bank holiday of...      1\n",
      "4    The selection on the menu was great and so wer...      1\n",
      "..                                                 ...    ...\n",
      "995  I think food should have flavor and texture an...      0\n",
      "996                           Appetite instantly gone.      0\n",
      "997  Overall I was not impressed and would not go b...      0\n",
      "998  The whole experience was underwhelming, and I ...      0\n",
      "999  Then, as if I hadn't wasted enough of my life ...      0\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "\n",
    "corpus = cleanup(dataset)\n",
    "\n",
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# fit to an array using fit_transform\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Set the label (1 for good, 0 for bad)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer.get_feature_names()\n",
    "x_array = X.toarray()\n",
    "print(x_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset[\"Liked\"]\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_array, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Training the Naive Bayes model on the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# # Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "stats =  accuracy_score(y_test, y_pred)\n",
    "modelLearn = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 42]\n",
      " [11 92]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captureString = \"this place sucked\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 1 entry dictionary similar to Reviews structure with Review:String    \n",
    "l = {\"Review\": [\"this place sucked\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into a dataframe\n",
    "dataOne = pd.DataFrame.from_dict(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Review\n",
      "0  this place sucked\n"
     ]
    }
   ],
   "source": [
    "print(dataOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup the dataframe\n",
    "oneline = cleanup(dataOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['place suck']\n"
     ]
    }
   ],
   "source": [
    "print(oneline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorizer.fit_transform(oneline)\n",
    "\n",
    "XOne_arr = x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Use classifier to predict the value\n",
    "y_pred = self.classifier.predict(XOne_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
