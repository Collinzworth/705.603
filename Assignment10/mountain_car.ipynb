{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mountain Car RL Project\n",
    "\n",
    "Aaron Collinsworth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from random_mc import random_mc\n",
    "\n",
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "\n",
    "max_steps = 10000000\n",
    "num_to_avg = 50\n",
    "\n",
    "num_steps = []\n",
    "for i in range(num_to_avg):\n",
    "    print(\"Episode {}\".format(i))\n",
    "    num_steps.append(random_mc(env, max_steps))\n",
    "\n",
    "avg_num_steps = sum(num_steps)/len(num_steps)\n",
    "\n",
    "print(\"Avg number of steps required {}\".format(avg_num_steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random mountain car uses random actions until it reaches goal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import mountain_car_func as mcf\n",
    "\n",
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "env.reset()\n",
    "\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "before_training = \"last_episodes.mp4\"\n",
    "video = VideoRecorder(env, before_training)\n",
    "\n",
    "# Define Q-learning function\n",
    "def QLearning(env, learning_rate, discount_factor, epsilon, num_episodes, print_interval=100):\n",
    "    \n",
    "    # Determine size of discretized state space\n",
    "    num_states = (env.observation_space.high - env.observation_space.low)*np.array([10, 100])\n",
    "    num_states = np.round(num_states, 0).astype(int) + 1\n",
    "    \n",
    "    # Initialize Q table\n",
    "    Q = np.random.uniform(low = -1, high = 1, \n",
    "                          size = (num_states[0], num_states[1], \n",
    "                                  env.action_space.n))\n",
    "    \n",
    "    # Initialize variables to track rewards\n",
    "    reward_list = []\n",
    "    avg_rewards = []\n",
    "    \n",
    "    # Calculate amount to reduce epsilon by\n",
    "    episode_decay = epsilon/num_episodes\n",
    "\n",
    "    # Run Q learning algorithm\n",
    "    for episode_num in range(num_episodes):\n",
    "                \n",
    "        Q, reward_list, epsilon = perform_episode(Q, episode_num, num_episodes, learning_rate, discount_factor,episode_decay, reward_list, avg_rewards, print_interval, video, epsilon)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return avg_rewards\n",
    "\n",
    "def perform_episode(Q_Table, episode_num, num_episodes, learning_rate, discount_factor,episode_decay, reward_list, avg_rewards, print_interval, video, epsilon):\n",
    "    \n",
    "    # Initialize parameters\n",
    "    episode_complete = False\n",
    "    total_reward = 0\n",
    "    reward = 0\n",
    "\n",
    "    # Reset Initial State\n",
    "    state = env.reset()\n",
    "    state_adj = mcf.discretize_state(env, state[0])\n",
    "\n",
    "    while episode_complete != True:\n",
    "\n",
    "        mcf.video_capture(env, video, num_episodes, episode_num)\n",
    "\n",
    "        action = mcf.select_action(Q_Table, env, state_adj, epsilon)\n",
    "\n",
    "        # Apply action to move to next state - obtain rewards\n",
    "        new_state, reward, episode_complete, truncated, info = env.step(action) \n",
    "\n",
    "        # Discretize new_state\n",
    "        new_state_adj = mcf.discretize_state(env, new_state)\n",
    "\n",
    "        Q_Table = update_q_table(Q_Table, state_adj, new_state, action, new_state_adj, episode_complete, learning_rate, discount_factor, reward)\n",
    "\n",
    "        # Update variables\n",
    "        total_reward = total_reward + reward\n",
    "        state_adj = new_state_adj\n",
    "\n",
    "    # Update epsilon\n",
    "    epsilon = epsilon - episode_decay\n",
    "    \n",
    "    # Track rewards\n",
    "    reward_list.append(total_reward)\n",
    "\n",
    "    mcf.print_avg_reward(episode_num, avg_rewards, reward_list, print_interval)\n",
    "\n",
    "    return Q_Table, reward_list, epsilon\n",
    "\n",
    "def update_q_table(Q_Table, state_adj, new_state, action, new_state_adj, episode_complete, learning_rate, discount_factor, reward):\n",
    "\n",
    "    # Terminal States\n",
    "    if episode_complete and new_state[0] >= 0.5:\n",
    "        Q_Table[state_adj[0], state_adj[1], action] = reward\n",
    "\n",
    "    # Update Q value for current state\n",
    "    else:\n",
    "        delta = learning_rate*(reward + discount_factor*np.max(Q_Table[new_state_adj[0], new_state_adj[1]]) - Q_Table[state_adj[0], state_adj[1],action])\n",
    "        Q_Table[state_adj[0], state_adj[1], action] = Q_Table[state_adj[0], state_adj[1], action] + delta\n",
    "\n",
    "    return Q_Table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter explaination:\n",
    "\n",
    "learning rate - controls the amount of adjustments made from each episode. Lower learning rates mean that the model will adjust in smaller intervals.\n",
    "\n",
    "discount factor - weight that controls how much future states impact the current state.\n",
    "\n",
    "epsilon - controls the exploration vs exploitation balance by committing to more greedy searches as time goes on.\n",
    "\n",
    "num_episodes - number of episodes to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.2\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.8\n",
    "num_episodes = 30\n",
    "\n",
    "# Run Q-learning algorithm\n",
    "rewards = QLearning(env, learning_rate, discount_factor, epsilon, num_episodes)\n",
    "\n",
    "# Plot Rewards\n",
    "plt.plot(100*(np.arange(len(rewards)) + 1), rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes')\n",
    "plt.savefig('rewards.jpg')     \n",
    "plt.close()  \n",
    "\n",
    "video.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the q-learning approach is able to learn and find a working policy in less moves than the random approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apologies for the lack of depth in explainations. I just wanted to prove I got the random portion and q-learning portion complete. I strugged with the deep q-learning part and ran out of time. Setting up the enviroment on windows was actually pretty hard and getting a visual inside a docker container was not working. I eventually was able to save it as a video. Please go easy on the grading lol."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
